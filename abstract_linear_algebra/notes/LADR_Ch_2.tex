% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass[11pt]{article} % use larger type; default would be 10pt
\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
\usepackage{clays_notes}


\newcommand\R[1]{\mathbf{R^{#1}}}
\newcommand\C[1]{\mathbf{C^{#1}}}
\newcommand\F[1]{\mathbf{F^{#1}}}
\newcommand\U{\mathbf{U}}
\newcommand\V{\mathbf{V}}
\newcommand\W{\mathbf{W}}
\newcommand\lc[3]{#1_1 #2_1 + \cdots + #1_{#3} #2_{#3}}
\newcommand\ls[2]{#1_1, \ldots, #1_{#2}}
\newcommand\spann[1]{\mathbf{span}(#1)}
\newcommand\set[1]{\{#1\}}
\newcommand\poly[1]{\mathcal{P}_{#1}(\F{})}
%\renewcommand\deg{\text{deg }}
\renewcommand\dim{\text{dim}}
\newcommand\0{\mathbf{0}}


% FONT STYLES

\title{Linear Algebra Done Right \\ Axler, Sheldon}
\author{Notes by:  \\ Clay Curry}
\date{}

\begin{document}

\section{Finite-Dimensional Vector Spaces}
Not all vector spaces come in the same size. Ituitively, this makes sense given the feeling of having more room to move about in the space of $\R{3}$ than the mere plane of $\R{2}$. In fact, although $\R{2}$ and $\R{3}$ contain infinitely many points, it can be shown that, in the everyday sense of "size", the size of $\R{3}$ is indeed bigger than $\R{2}$ (some infinities are bigger than others). Linear algebra focues mainly on the properties of finite-dimensional vector spaces, which are introduced in this chapter. 

The basic tools needed to understand the structure of functions (or mappings) between finite-dimensional vector spaces are also defined in this chapter, namely span, linear independence, basis, and dimension. 

\subsection{Span and Linear Independence}
Adding up scalar multiples of vectors in a list gives what is called a \textbf{linear combination} of the list.

\subsubsection{Linear Combinations and Span}

\definition{Linear Combination}
{
A \textbf{linear combination} of a list $\ls{v}{m}$ of vectors in $\V$ is a vector of the form
\mathdiv{\lc{a}{v}{m}}
where $\ls{a}{m} \in \F{}$.
}

Some mathematicians use the term \textbf{linear span} or \textbf{linear hull}, which means the same thing as span. The concept of span is particularly useful for describing the cardinality or ``size" of a vector space. For dealing with notions of ``sizes'' of infinite sets, words like ``larger" and ``smaller" are defined in terms of subset relations. In other words, a set $\mathbf{A}$ is bigger than a set $\mathbf{B}$ implies that $\mathbf{B \subset A}$.

\definition{Span, Generating Set}
{
The set of all linear combinations of a list of vector $\ls{v}{m} \in \V$ is called the \textbf{span} of $\ls{v}{m}$, denoted $\spann{\ls{v}{m}}$. In other words
\mathdiv{\spann{\ls{v}{m}} = \set{\lc{a}{v}{m} : \ls{a}{m} \in \F{}}.}
To express that a vector space $\V$ is a span of a set $\ls{v}{m}$, one commonly uses the following phrases:
\points
{$\ls{v}{m}$ \textbf{spans} $\V$;}
{$\ls{v}{m}$ \textbf{generates} $\V$;}
{$\V$ is spanned by $\ls{v}{m}$;}
{$\V$ is generated by $\ls{v}{m}$;}
{$\ls{v}{m}$ is a spanning set of $\V$;}
{$\ls{v}{m}$ is a generating set of $\V$;}
The span of the empty list $()$ is defined to be zero.
}

In general, the substructure (subgroup, subring, subalgebra, subspace) generated by some list of elements in a mathematical structure is the smallest substructure containing at least those elements in the list.

\theorem{Span is the smallest containing subspace}
{
The span of a list of vectors in $V$ is the smallest subspace of $V$ containing all the vectors in the list.
}
{
To show that $\spann{\ls{v}{m}}$ is the ``smallest" subspace of $V$ containing $\ls{v}{m}$ is to show that $\spann{\ls{v}{m}}$ is a subspace \textit{and simultaneously} a subset of any subspace of $V$ containing $\ls{v}{m}$. Take any list $\ls{v}{m} \in V$ and $\lambda, \ls{a}{m}, \ls{b}{m} \in \F{}$. \\

First $\spann{\ls{v}{m}}$ is demonstrated to be a subspace. 
\begin{adjustwidth}{1cm}{}
1.	The additive identity $\0 \in \spann{\ls{v}{m}}$ because $0 v_1 + \cdots + 0 v_m = \0 + \cdots + \0 = \0$.\\
2.	The $\spann{\ls{v}{m}}$ is closed under addition because $(\lc{a}{v}{m}) + (\lc{b}{v}{m}) = a_1 v_1 + b_1 v_1 + \cdots + a_m v_m + b_m v_m = (a_1 + b_1) v_1 + \cdots + (a_m + b_m) v_m \in \spann{\ls{v}{m}}$ because $(a_j + b_j) \in \F{}$ for every $j \in \{1, \ldots, m\}$.\\ 
3. The $\spann{\ls{v}{m}}$ is closed under scalar multiplication because $\lambda (\lc{a}{v}{m}) = \lambda(a_1 v_1) + \cdots + \lambda(a_m v_m) = (\lambda a_1) v_1 + \cdots + (\lambda a_m) v_m \in \spann{\ls{a}{m}}$ because $(\lambda a_j) \in \F{}$. \\
\end{adjustwidth}

Next $\spann{\ls{v}{m}}$ is demonstrated to be a subset of any subspace $U \subseteq V$ containing $\ls{v}{m}$. 
\begin{adjustwidth}{1cm}{}
1. $\ls{v}{m} \in U \implies \lc{a}{v}{m} \in U$ because $U$ is a subspace. \\
2. $\spann{\ls{v}{m}} \coloneqq \{ \lc{a}{v}{m} : a \in \F{}\}$ \\
3. $\spann{\ls{v}{m}} \subseteq U$.

\end{adjustwidth}
}


\example{Natual Basis}
{
Suppose $n \in \mathbf{N}$. Show that
\mathdiv{(1, 0, \ldots, 0), (0, 1, 0, \ldots,0), \ldots, (0, \ldots, 0, 1)}spans $\F{n}$. Here the $j^{\text{th}}$ vector in the list above is the $n$-tuple with 1 in the $j^{\text{th}}$ slot and $0$ in all the other slots.\\
Suppose $(\ls{x}{n}) \in \F{n}$. Then
\mathdiv{(\ls{x}{n}) = (x_1, 0, \ldots, 0) + \cdots + (0, \ldots, 0, x_n) = x_1(1, 0, \ldots, 0) + \cdots + x_n(0, \ldots, 0, 1)}
Thus $(\ls{x}{n}) \in \spann{(1, 0, \ldots, 0), \ldots, (0, \ldots, 0, 1)}$, as desired. 
}

Now we can make one of the key definitions in linear algebra. Recall that every list, by definition, has a finite length.
\definition{Finite Dimensional Vector Space}
{A vector space is called \textbf{finite-dimensional} if some list of vectors in it spans the space.}
The above example shows that $\F{n}$ is a finite-dimensional vector space for every positive integer $n$.

\definition{Polynomial, $\poly{}$}
{
A function $p : \F{} \to \F{}$ is called a \textbf{polynomial} with coefficients in $\F{}$ if there exists $a_0, \ls{a}{m} \in \F{}$ such that
\mathdiv{p(z) = a_0 + a_1 z + a_2 z^2 + \cdots + a_m z^m}
for all $z \in \F{}$.
\points
{$\poly{}$ is the set of all polynomials with coefficients in $\F{}$.}
}

With the usual operations of addition and scalar multiplication, $\poly{}$ is a vector space over $\F{}$. As such, $\poly{}$ is a subspace of $\F{\F{}}$.

It will later be shown that the coefficients of the polynomial uniquely determine the polynomial.
\definition{degree of a polynomial, deg p}
{
\points
{A polynomial $p \in \poly{}$ is said to have \textit{\textbf{degree} m} if there exists scalars $a_0, \ls{a}{m} \in \F{}$ with $a_m \ne 0$ such that
\mathdiv{p(z) = a_0 + a_1 z + a_2 z^2 + \cdots + a_m z^m}
for all $z \in \F{}$. If $p$ has degree $m$, we write $\deg p = m$.}
{The polynomial that is identically 0 is said to have degree $- \infty$.}
}

We use the convention that $- \infty < m$, which means that the polynomial $0$ is in $\poly{m}$.
\definition{$\poly{m}$}
{For any $m \in \mathbb{N}$, $\poly{m}$ denotes the set of all polynomials with coefficients in $\F{}$ and degree at most $m$.}


\example{Finite-dimensional vector space}
{$\poly{m}$ is a finite dimensional vector space for each non-negative integer $m$.}

\definition{Infinite-dimensional vector space}
{A vector space is called \textbf{infinite-dimensional} if it is not finite dimensional.}

\example{Infinite-dimensional vector space}
{$\poly{}$ is infinite dimensional.}

\subsubsection{Linear Independence}
Suppose $\ls{v}{m} \in V$ and $v \in \spann{\ls{v}{m}}$. By the definition of span, there exists $\ls{a}{m} \in \F{}$ such that
\mathdiv{v=\lc{a}{v}{m}.}

Consider the question of whether the choice of scalars in the expression is unique. Suppose $\ls{c}{n} \in \F{}$ is another set of scalars such that
\mathdiv{v = \lc{c}{v}{m}.}

Subtracting the last two equations, we have
\mathdiv{0 = (a_1 - c_1) v_1 + \cdots + (a_m - c_m) v_m.}

If the only way to write $0$ as a linear combination of $\ls{v}{m}$ is by taking $0$ for each scalar then, $a_j = c_j$ (the choice of scalars for each vector in $\spann{\ls{v}{m}}$ must be unique). This situation is so important that we give it a special name, linear independence.

\definition{Linearly Independent}
{\points
{A list $\ls{v}{n}$ of vectors in $V$ is called \textbf{linearly independent} if the only choice of $\ls{a}{n} \in \F{}$ that makes $\lc{a}{v}{n}$ equal to $0$ is $a_1 = \cdots = a_n = 0$.}
{The empty list () is also declared to be linearly independent.}
}

\definition{Linearly dependent}
{\points{A list $\ls{v}{n}$ of vectors in $V$ is called \textbf{linearly dependent} if it is not linearly independent.}}

The following theorem repeatedly proves to be useful in solving many theorems deeper in the subject. 
\theorem{Linear Dependence Lemma}
{
Suppose $\ls{v}{m}$ is a linearly dependent list in $V$. Then there exists $j \le m$ such that the following hold:
\points
{$v_j \in \spann{\ls{v}{j-1}}$;}
{If the $j^{\text{th}}$ term is removed from $\ls{v}{m}$, the span of the remaining list equals $\spann{\ls{v}{m}}$.}
}
{The following procedure demonstrates the first proposition. Assuming $\ls{v}{m}$ is linearly dependent, by definition,\mathdiv{\exists \ls{a}{m} \in \F{} \text{ not all } 0 \text{ such that } \0 = \lc{a}{v}{m}.}
For any such combination, select the highest value of $j \in \{1, \ldots, m \}$ such that $a_j \ne 0$. Notice, \mathdiv{-a_jv_j = \lc{a}{v}{j-1} \implies v_j = \frac{a_1}{-a_j}v_1 + \cdots + \frac{a_{j-1}}{-a_j}v_{j-1} \text{ (which is possible since $a_j \ne 0$)}}Hence there exists $j \le m$ such that, \mathdiv{v_j \in \spann{\ls{v}{j-1}}.}

For the second proposition, the following procedure demonstrates that $v_j$ can be removed without changing the span. Building on the previous result,\mathdiv{v_j \in \spann{\ls{v}{j-1}} \implies \exists \ls{b}{j-1} \in \F{} \text{ such that } v_j = \lc{b}{v}{j-1}.}This situation means any element $w \in \spann{\ls{v}{m}}$, we have,
\begin{align*} 
w &= \lc{a}{v}{j-1} + a_j \mathbf{v_j} + a_{j+1} v_{j+1} + \cdots + a_m v_m \\
&= \lc{a}{v}{j-1} + a_j \mathbf{(\lc{b}{v}{j-1})} + a_{j+1} v_{j+1} + \cdots + a_m v_m \\
w &= (a_1+\mathbf{a_jb_1)v_1} + \cdots + (a_{j-1}+\mathbf{a_jb_{j-1}) v_{j-1}} + a_{j+1} v_{j+1} + \cdots + a_m v_m
\end{align*}
This bidirectional procedure demonstrates that \mathdiv{\spann{\ls{v}{m}} = \spann{v_1, \ldots, v_{j-1}, v_{j+1}, \ldots v_m}.}
}
\newpage

\theorem{Length of a linearly independent list $\le$ length of spanning list}
{
In a finite-dimensional vector space, the length of every linearly indepenedent list of vectors is less than or equal to the length of every spanning list of vectors.
}
{
Suppose $\ls{v}{n}$ is linearly independent in $V$ and $\ls{w}{m}$ spans $V$. Since $\ls{w}{m}$ spans $V, \exists \ls{a}{m},b_1 \in \F{}$ where \mathdiv{v_1 = \lc{a}{w}{m} \implies \0 = b_1 v_1 + \lc{a}{w}{m},}which is linearly dependent. By the Linear Dependence Lemma, it is possible to discover one $w_j \in \spann{v_1, w_1, \ldots, w_{j-1}}$ and remove it, resulting in one $v$ being added and one $w$ being removed, while the resulting list (of length $m$) continues to span $V$.

By the second part of the Linear Dependence Lemma, there exists scalars such that \mathdiv{v_j = b_1 v_1 + \cdots + a_m w_m \implies \0 = b_1 v_1 + \cdots + b_j v_j + \cdots + a_m w_m}By the Linear Dependence Lemma, there exists one vector that can be removed while leaving the span (all of $V$) unchanged. Since the $v$'s are linearly independent, the above equality requires that at least one $a_j$ will be non-zero. 
The process of placing one $v$, producing a dependent list, and removing one $w$ while leaving the span unchanged continues until either we run out of $v$'s or $w$'s. If we run out of $w$'s, we necessarily also run out of $v$'s (or else the remaining $v$'s would be in the span of the previous $v$'s, which contradicts our assumption). 

Hence $m \le n$, as desired.
}


\subsection{Bases}
The following section derives crucial theorems built on concepts of linear independence and span.

\definition{Basis}
{A \textbf{basis} of $\V$ is a list of vectors in $\V$ that is linearly independent and spans $\V$.}

The next result explains why bases are useful. 

\theorem{Criterion for basis}
{A list $\ls{v}{n}$ of vectors in $\V$ is a basis of $\V$ if and only if every $v \in \V$ can be written uniquely in the form
\mathdiv{v = \lc{a}{v}{n}}
where $\ls{a}{n} \in \F{}$
}
{
Suppose $\ls{v}{m}$ is a basis for $V$ and
\begin{align*}
&v = \lc avm = \lc cvm\\
\implies& \0 = (a_1 - c_1) v_1 + \cdots + (a_m - c_m) v_m \\
\implies& a_1 = c_1 \cdots = a_m = c_m \\
\implies& \text{ there is a unique way to write } v.
\end{align*}

For the converse, $\forall v \in V$, there exists a unique list of scalars $\ls am,$ such that
\begin{align*}
&\textbf{ 1. Spanning Condition } &v &= \lc avm \\
&\textbf{ 2. Linear Independence } &\0 &= \lc avm  \iff a_1 = \cdots = a_m = 0
\end{align*}

Hence, $\ls{v}{m}$ is a basis for $V$.
}
\theorem{Spanning list contains a basis}
{Every spanning list in a vector space can be reduced to a basis of the vector space.}
{
Suppose $\ls{w}{m}$ spans $V$ and is dependent. The Linear Dependence Lemma states that we can remove any element in the span of the previous vectors without altering the span of the entire list. It has already been shown that the length of a linearly independent list $\le$ length of spanning list. Therefore, if $\ls{v}{n}$ is linearly independent in $V$, then reducing $\ls{w}{m}$ to a basis requires removing no more than $m-n$ vectors from $\ls{w}{m}$.
}

\theorem{Basis of finite-dimensional vector space}
{Every finite-dimensional vector space has a basis}
{
Take any finite-dimensional vector space $V$, and suppose $\ls{v}{n}$ generates $V$. It has already been shown every spanning list in a vector space can be reduced to a basis of the vector space. Since $\ls{v}{n}$ can be reduced to a basis, $V$ has a basis. 
}

\theorem{Linearly independent list extends to a basis}
{Every linearly independent list of vectors in a finite-dimensional vector space can be extended to a basis of the vector space.}
{
Take any finite-dimensional vector space $V$, and suppose $\ls vn$ generates $V$ and $\ls{w}{m}$ is independent in $V$. Using the same substitution procedure as the Linear Dependence Lemma, it is possible to substitute each $w_j$ for exactly $m$ $v$'s to create a new list that includes each $w$ followed by enough $v$'s to make the new list span $V$. This can be reduced to a basis.
}

\theorem{Every subspace of $\V$ is part of a direct sum equal to $\V$}
{Suppose $V$ is finite-dimensional and $U$ is a subspace of $V$. Then there is a subspace $W$ of $V$ such that $V = U \oplus W$.}
{
Suppose $U \text{ is a subspace of } V$ 
\begin{align*} &\implies U \text{ is finite-dimensional}\\
&\implies U \text{ has a basis (call it $\ls uk$)}\\
&\implies \ls uk \text{ is linearly independent in $V$}\\
&\implies \ls uk \text{ can be extended to a basis $\ls uk, \ls wl$ of $V$} \\
&\implies \spann{\ls wl} \text{ form a subspace $W$ in $V$}
\end{align*}

Since $\ls uk, \ls wl$ is linearly independent in $V$, the only way to write $\0$ as a sum of $\lc auk + \lc bwl$ is by taking $a_1 = \cdots = a_k = b_1 = \cdots = b_l = 0$. 

In other words, the only way to write $\0$ as the sum of $\mathbf{u} \in U$ and $\mathbf{w} \in W$ is by taking $\mathbf{u} = \mathbf{w} = 0$. Hence, $U \oplus W$ is a direct sum.
}
\newpage

\subsection{Dimension}

How should dimension be defined? A reasonable definition of dimension should force $\F{n}$ to be $n$. Notice that the standard basis of $\F{n}$, \mathdiv{(1, 0, \ldots, 0), (0, 1, 0, \ldots,0), \ldots, (0, \ldots, 0, 1)}
has length $n$. It is therefore tempting to define the basis as the length of a basis.

\theorem{Basis length does not depend on basis}
{Any two bases of a finite-dimensional vector space have the same length.}
{
Suppose $V$ is finite-dimensional and $B_1$ and $B_2$ are bases in $V$. 
\begin{align*}
&B_1 \text{ is linearly independent in } V \text{ and } B_2 \text{ spans } V\\
&\implies \text{the length of } B_1 \le \text{ the length of } B_2
\end{align*}

Also,
\begin{align*}
&B_2 \text{ is linearly independent in } V \text{ and } B_1 \text{ spans } V\\
&\implies \text{the length of } B_2 \le \text{ the length of } B_1
\end{align*}

Hence\mathdiv{\text{the length of } B_1 = \text{the length of } B_2.}

}

\definition{Dimension, $\dim V$}
{\points
{The \textbf{dimension} of a finite-dimensional vector space is the length of any basis of the vector space.}
{The dimension of $V$ (if $V$ is finite-dimensional) is denoted by $\dim{V}$.}
}

\theorem{Dimension of a subspace}
{If $\V$ is finite-dimensional and $\U$ is a subspace of $\V$, then $\dim{\U} \le \dim{\V}$.}
{Suppose $\V$ has dimension $n < \infty$ and $\U$ is a subspace of $\V$. Then $\U$ is finite-dimensional and has a basis $\ls um$. It has already been shown that the length of a linearly independent list is less than or equal to a spanning list. Since $\ls um$ is linearly independent in $\V$ and $\V$ has a basis of length $n$, $m \le n \implies \dim{\U} \le \dim{\V}$.}

The following theorems show that linearly independent or spanning lists of the right length can be known to be a basis.

\theorem{Linearly independent list of the right length is a basis}
{Suppose $\V$ is finite-dimensional. Then every linearly indepenedent list of vectors in $\V$ with length $\dim{\V}$ is a basis of $\V$.}
{Suppose $\V$ has dimension $n < \infty$ and $\ls vn$ is independent in $\V$. Since every linearly independent list can be extended to a basis, we can adjoin vectors to $\ls vn$ to make it a basis. Since a basis requires $n$ elements, no elements need to be added so that $\ls vn$ is a basis.}

\theorem{Spanning list of the right length is a basis}
{Suppose $\V$ is finite-dimensional. Then every spanning list of vectors in $\V$ with length $\dim{\V}$ is a basis of $\V.$}
{Suppose $\V$ has dimension $n < \infty$ and $\ls vn$ spans $\V$. Since every spanning list can be extended to a basis, we can reduce the list to make it a basis. Since a basis requires $n$ elements, no elements need to be removed so that $\ls vn$ is a basis.}

\theorem{Dimension of a sum}
{If $U_1$ and $U_2$ are subspaces of a finite dimensional vector space, then
\mathdiv{\dim(U_1 + U_2) = \dim U_1 + \dim U_2 - \dim (U_1 \cap U_2)}
}
{}

\end{document}