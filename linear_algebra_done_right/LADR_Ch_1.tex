% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass[11pt]{article} % use larger type; default would be 10pt
\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
\usepackage{clays_notes}

\newcommand\R[1]{\mathbf{R^{#1}}}
\newcommand\C[1]{\mathbf{C^{#1}}}
\newcommand\F[1]{\mathbf{F^{#1}}}
\newcommand\U{\mathbf{U}}
\renewcommand\u{\mathbf{u}}
\newcommand\V{\mathbf{V}}
\newcommand\W{\mathbf{W}}


% FONT STYLES

\title{Linear Algebra Done Right \\ Axler, Sheldon}
\author{Notes by:  \\ Clay Curry}
\date{}

\begin{document}

\section{Vector Spaces}

Linear algebra is the study of linear maps on finite-dimensional vector spaces. Vector spaces are defined in this chapter, and their basic properties are developed. Vector spaces are a generalization of the description of a plane using two coordinates, as published by Descartes in 1637. 

\subsection{$\R{n}$, $\C{n}$, and $\F{n}$}

\subsubsection{Complex Numbers}
Complex numbers were invented so that we can take square roots of negative numbers. The idea is to assume we have a square root of $-1$, denoted $i$, that obeys the usual rules of arithemetic. Here are the formal definitions:

\definition
{Complex Number}
{
A \textbf{complex number} is an ordered pair $(a,b) \in \mathbf{R}^2$, denoted $a + bi$. 
	\points
	{
	The set of all complex numbers is denoted by $\C{ }$: 
	\mathdiv{\C{ } = \{a+bi : a,b \in \R{ } \}}
	}
}

Relations on any collection of mathematical objects are the mathematical tools used to \textit{describe} the additional structure or relationships \textit{defined/discovered} between those objects. For example, the relation $1 < 2$ is true because $<$ semantically depends on the "ordering" in $\R{}$ that defines 1 to be "less than" 2. 

Operations are a special kind of kind of function that map zero or more input values from a set $S$ to a well-defined output in $S$. The reason we consider the following statements to be "Properties of Complex Numbers", rather than "Properties of Operations on Complex Numbers" is because operations such as addition, multiplication, negation, and exponentiation describe actual structure and relationships between numbers in $\C{}$.

\definition{Operations on Complex Numbers}
{
\textbf{Addition and multiplication} on $\C{ }$ are defined by
\mathdiv{ (a + bi) + (c + di) \equiv (a + c) + (b + d)i }
\mathdiv{(a + bi)\cdot(c+di) \equiv (ac - bd) + (ad + bc)i }
}

The following properies are proven using the familiar properites of real numbers and the definition of complex addition and multiplication.

\property{Properties of Complex Numbers}
{
\points
{\textbf{Commutativity : } $\alpha + \beta = \beta + \alpha$ and $\alpha\beta = \beta\alpha$ for all $\alpha, \beta \in \mathbf{C}$}
{\textbf{Associativity : } $(\alpha + \beta) + \lambda = \alpha + (\beta + \lambda)$ and $(\alpha \cdot \beta)\cdot \lambda = \alpha \cdot (\beta \cdot \lambda)$ for all  $\alpha, \beta, \lambda \in \mathbf{C}$}
{\textbf{Identities : } $\lambda + e_+ \equiv \lambda + 0 \equiv \lambda$ and $\lambda \cdot e_\cdot \equiv \lambda \cdot 1 \equiv \lambda$ for all $\lambda \in \mathbf{C}$}
{\textbf{Additive Inverse : } $\forall \alpha \in \mathbf{C}, \exists \beta \in \mathbf{C}$, such that $\alpha + \beta = e_+ = 0$}
{\textbf{Multiplicative Inverse : } $\forall \alpha \in \mathbf{C}, \exists \beta \in \mathbf{C}$, such that $\alpha \cdot \beta = e_\cdot = 1$}
{\textbf{Distributive Property} $\lambda \cdot (\alpha + \beta) = \lambda \cdot \alpha + \lambda \cdot \beta$, for all $\alpha, \beta, \lambda \in \mathbf{C}$}
}
\clearpage

The following operations are defined for convenience of thinking and proof-writing. They do not ordain a set with any additional structure beyond what is already defined by their parent operations.

\definition{Constructed Operations on $\C{}$: Subtraction and Division}
{
Let $\alpha, \beta \in \C{}$.
	\points
	{Let $-\alpha$ denote the additive inverse of $\alpha$.  Thus, $-\alpha$ is the unique element of $\C{}$ such that $\alpha+ (-\alpha) = 0$.}
	{Subtraction on $\C{}$ is defined by \mathdiv{\beta - \alpha = \beta + (-\alpha)}}
	{Let $(1/\alpha)$ denote the multiplicative inverse of $\alpha$.  Thus, $(1/\alpha)$ is the unique element of $\C{}$ such that $\alpha \cdot (1/\alpha) = 1$.}
	{Division on \textbf{C} is defined by \mathdiv{\beta/\alpha = \beta \cdot (1/\alpha)}}
}

Throughout these notes, $\F{}$ stands for either $\R{}$ or $\C{}$. The letter $\F{}$ is used because $\R{}$ and $\C{}$ are examples of the algebraic structure known as a \textbf{field}.

\subsubsection{From $\F{}$ to $\F{n}$}

\definition{List, length}
{
Suppose $n \in \mathbb{N}$. A \textbf{list} of \textbf{length} $n$ is an ordered collection of $n$ elements (e.g. numbers, other lists, functions, more abstract entities) separated by commas and--optionally--surrounded by parentheses. Many mathematicians call a list of length $n$ an $\mathbf{n}$\textbf{-tuple}.
	
	\points
	{Two lists are \textbf{equal} means they have the same length and the same elements in the same order.}
	{$\F{n}$ is the set of all lists of length $n$ of elements of $\F{}$. \mathdiv{\F{n} = \{ (x_1, \ldots, x_n) : x_1, \ldots, x_n \in \F{} \}}}
	{For $(x_1, \ldots, x_n) \in \F{n}$ and $j \in \{1, \ldots, n\}$, we say that $x_j$ is the $j^\text{th}$ \textbf{coordinate} of $(x_1, \ldots, x_n)$.}
}

Having defined a basic set of mathematical objects, $\F{n}$, it is possible to define operations that describe the structure existing between them.

\definition{Addition in $\F{n}$}
{
The \textbf{sum} of two elements $\mathbf{x, y} \in \F{n}$, denoted $\mathbf{x + y}$ (and properly read in all mathematical settings as "\textbf{the sum of $\mathbf{x}$ and $\mathbf{y}$}"), is computed by adding the cooresponding coordinates.
\mathdiv{(x_1, \ldots, x_n) + (y_1, \ldots, y_n) \equiv (x_1 + y_1, \ldots, x_n + y_n) }
}

\definition{0}
{Let \textbf{0} denote the additive identity in $\F{n}$, that is, the list of length $n$ whose coordinates are all 0: \mathdiv{\mathbf{0} = (0, \ldots, 0).}}

\definition{Additive Inverse}
{
For $x \in \F{n}$, the additive inverse of $x$, denoted $-x$, is the vector $-x \in \F{n}$ that maps the vector $x$ under addition to the additive identity of $\F{n}$: \mathdiv{x + (-x) = 0}.
}

Commutativity is a symmetric property of an operation. It denotes the invariance between permutations of operands under an operation.

\theorem{Commutativity of addition in $\F{n}$}
{If $x, y \in \F{n}$, then $x + y = y + x$.}
{
Choose any $x, y \in \F{}$.
	\begin{align*}
	x + y &= (x_1, \ldots, x_n) + (y_1, \ldots, y_n) \\
		&= (x_1 + y_1, \ldots, x_n + y_n) \\
		&= (y_1 + x_1, \ldots, y_n + x_n) \\
		&= (y_1, \ldots, y_n) + (x_1, \ldots, x_n) \\
		&= y + x
	\end{align*}
Hence, for any $x, y \in \F{} \implies x + y = y + x$
}

\vspace{10 pt}
In previous words, \textbf{``the sum of $\mathbf{x}$ and $\mathbf{y}$"} is the same as \textbf{``the sum of $\mathbf{y}$ and $\mathbf{x}$"}.
\clearpage

\subsection{Definition of Vector Space}

The motivation for the definition of a vector space comes from properties of addition and scalar multiplication in $\F{n}$:
	\points
	{$+$ is commutative, associative, has an identity, pair each element with an additive inverse.}
	{$\cdot$ is associative, has an identity.}
	{$+$ and $\cdot$ are connected by distributive properties. }

\definition{Addition, Scalar Multiplication}
{
\points
{An \textbf{Addition} on a set $V$ is a function that assigns an element $u + v \in V$ to each pair of elements $u, v \in V$.}
{A \textbf{Scalar Multiplication} on a set $V$ is a function that assigns an element $\lambda v \in V$ to each $\lambda \in \textbf{F}$ and each $v \in V$.}
}

Formal definition for a Vector Space:

\definition{Vector Space}
{
A \textbf{Vector Space} is a set $\V$ with an addition $+ : \V \times \V \rightarrow \V$ on $\V$ and scalar multiplication $\cdot : \F{} \times \V \rightarrow \V$ on $\V$ such that the following properties hold:
	\points
	{\textbf{Commutativity} : $u + v = v + u \text{ for all } u,v \in \V$;}
	{\textbf{Associativity} : $(u + v) + w = u + (v + w) \text{ and } a (bv) = (ab) v \text{ for all } u, v, w \in \V \text{ and all } a, b \in \mathbf{F}$;}
	{\textbf{Additive Identity} : $\exists \hspace{4 pt} \mathbf{0} \in \V\text{, such that } \mathbf{0} + v = v,$ for all $v \in \V$;}
	{\textbf{Additive Inverse} : For all $v \in \V$, there exists $-v \in \V$ such that $v + (-v) = \mathbf{0}$;}
	{\textbf{Multiplicative Identity} : $\mathbf{1} \cdot v = v$ as expected for $\mathbf{1} \in \mathbf{F}$;}
	{\textbf{Distributive Properties} : Addition and Scalar Multiplication satisfy the following two relations:
	\mathdiv{a (v + u) = av + au \text{ for all } a \in F \text{ and } u, v \in \V, \\}
	\mathdiv{(a + b) v = av + bv \text{ for all } a, b \in F \text{ and } v \in \V.}
	}
}

Since the properties of a vector space depend on properties of scalar multiplication, which itself depends on the choice of $\F{}$, the phrase \textbf{``$\V$ is a vector space over $\F{}$"} indicates that $\V$ is a vector space and, for the situation at hand, the choice of scalar field is $\F{}$.

\definition{Real Vector Space, Complex Vector Space}
{
\points
{A vector space over $\R{}$ is called a \textbf{real vector space}.}
{A vector space over $\C{}$ is called a \textbf{complex vector space}.}
}

The following geometric language is defined for convenience of thinking. \textbf{Linear algebra} studies the \textit{\textbf{algebra}} of vector spaces; intuition depending greatly on geometric insight will significantly hamper the development of algebraic insight. 

\clearpage

Common geometric notions  like ``angle", ``orientation", and ``distance" between points and vectors (unless we consider a vector space with an \textit{inner product})  do not ordain a vector space with additional structure beyond what is already defined by the basic operations. For dealing with these ideas in linear algebra, the \textit{inner product} and \textit{inner product spaces} is a later topic of discussion.

\definition{Vector, Point}
{Elements of a vector space are called \textbf{vectors} or \textbf{points}.}

The following operation is defined for convenience of thinking and proof-writing. It does not ordain a set with any additional structure beyond what is already defined by its parent operation.

\definition{Subtraction}
{
Let $v, w \in \V$. Then 
	\points
	{$-v$ denotes the additive inverse of $v$;}
	{$w-v$ is defined to be $w + (-v)$.}
}

There are many more examples of vector spaces than $\F{n}$.

\definition{$\F{S}$}
{
If $S$ is a set, then $\mathbf{F}^S$ denotes the set of functions $\{ f : f : S \rightarrow \mathbf{F}\}$. For $f, g \in \mathbf{F}^S$, addition and scalar multiplication are defined by:
	\points
	{\mathdiv{(f+g)(x) = f(x) + g(x)}    for all $x \in S$.}
	{\mathdiv{(\lambda \cdot f)(x) = \lambda \cdot f(x)}    for all $x \in S$.}
It is easy to verify that $\F{S}$ is a vector space.
}

The following theorems hold for all vector spaces.


\theorem{Unique Additive Identity}
{Every vector space has a unique additive identity.}
{
Suppose $0$ and $0'$ are additive identities in $V$.
Suppose $0$ and $0'$ are additive identities in $\V$.
\mathdiv{
	0 = 0 + 0' = 0' + 0 = 0'
}
where the first equality holds because $0$ hold its identity under adddition with $0'$; the second equality holds by the commutative property of addition in vector spaces, and the final equality holds because $0'$ hold its identity under adddition with $0$. Hence, any two elements that are additive identities in $\V$ are, in fact, equal. Every vector space has a unique additive identity.
}
\clearpage

\theorem{Unique Additive Inverse}
{Every element in a vector space has a unique additive inverse.}
{
Take any $u, v, w \in \V$ where $u + v = 0$ and $u + w = 0$. Then
	\begin{align*}
	v 	&= v + 0 \\
		&= v + (u + w) \\
		&= (v + u) + w \\
		&= (u + v) + w \\
		&= 0 + w \\
		&= w \\
	\end{align*}
Hence, any two elements that are additive inverses of some $u \in \V$ are, in fact, equal. Every element in a vector space has a unique additive inverse.
}

\theorem{The Annihilating Scalar}
{$0 \cdot v = 0 \in \V$ for every $v \in \V$.}
{
Take any $v \in \V$. Notice 
\mathdiv{0 \cdot v = (0 + 0) \cdot v = 0 \cdot v + 0 \cdot v.}
Since every element in $\V$ has an additive inverse, the additive inverse of $0 \cdot v$ can be added to both sides, resulting in the following sum, 
\mathdiv{\mathbf{0} = 0 \cdot v}
Hence, $0 \cdot v = 0 \in \V$ for every $v \in \V$. 
}

\theorem{Scaling the Additive Identity}
{$a \cdot 0  = 0 \in \V$ for every $a \in \textbf{F}$.}
{
Take any $a \in \F{}$. Notice \mathdiv{a \cdot 0 = a \cdot (0 + 0) = a \cdot 0 + a \cdot 0.}
Since every element in $\V$ has an additive inverse, the additive inverse of $a \cdot 0$ can be added to both sides, resulting in the following sum,
\mathdiv{\mathbf{0} = a \cdot 0}
Hence, $a \cdot 0  = 0 \in \V$ for every $a \in \textbf{F}$.
}

\theorem{The number (-1) as a Scalar}
{$(-1) \cdot v = -v$ for every $v \in \V$.}
{
By the annihilating scalar theorem, for any $v \in \V$, 
\mathdiv{\mathbf{0} = 0 \cdot v = (1 + (-1)) \cdot v = 1 \cdot v + (-1) \cdot v = v + (-1) \cdot v \implies -1 \cdot v = -v} 
Hence, $(-1) \cdot v = -v$ for every $v \in \V$.
}

%%%%%%%%%%%%%%%%
% Section 1.3 : Subspaces
%%%%%%%%%%%%%%%%
\clearpage
\subsection{Subspaces}

The concept of subspaces greatly increases the richness of the subject as a tool for discovery.

\definition{Subspace}
{A subset $\U \subset \V$ over $\F{}$ is called a subspace of $\V$ if $\U$ is also a vector space over $\F{}$.}

The next, very important, result gives an easy way to check whether a subset of a vector space is a subspace.
\theorem{Conditions for a Subspace}
{
A subset $\U \subset \V$ is a subspace of $\V$ over \textbf{F} if and only if $\U$ satisfies the following:
	\points
	{\textbf{Additive Identity: }$0 \in \U$;}
	{\textbf{Closure Under Addition: } $u, w \in \U \implies u + w \in \U$;}
	{\textbf{Closure Under Scalar Multiplication: } $u \in \U \implies \lambda \cdot u \in \U$ for every $\lambda \in \textbf{F}$.}
}
{
The forward implication is true by definition.
To demonstrate the converse, consider any $u, v \in \U \subset \V$. Since $u, v$ are, inherently, elements of $\V$, which is a vector space, the following statements are true for the operations of addition and scalar multiplication.
	\points
	{\textbf{Commutativity} : $u + v = v + u \text{ for all } u,v \in \U$;}
	{\textbf{Associativity} : $(u + v) + w = u + (v + w) \text{ and } a (bv) = (ab) v \text{ for all } u, v, w \in \U \text{ and all } a, b \in \mathbf{F}$;}
	{\textbf{Multiplicative Identity} : $\mathbf{1} \cdot v = v$ as expected for $\mathbf{1} \in \mathbf{F}$;}
	{\textbf{Distributive Properties} : Addition and Scalar Multiplication satisfy the following two relations:
	\mathdiv{a (v + u) = av + au \text{ for all } a \in \F{} \text{ and } u, v \in \U, \\}
	\mathdiv{(a + b) v = av + bv \text{ for all } a, b \in \F{} \text{ and } v \in \U.}
	}

For the above properties to be attrubuted to $\U$, addition and scalar multiplication for members of $\U$ should always produce another member of $\U$.

The following guarentees that $\U$ is a non-empty set, and it satisfies one of the two remaining required properties for $\U$ to be a vector space ($\exists \hspace{4 pt} \mathbf{0} \in \V\text{, such that } \mathbf{0} + v = v,$ for all $v \in \V$): 
\mathdiv{\text{\textbf{Additive Identity} : }\exists \hspace{4 pt} \mathbf{0} \in \U \text{, such that } \mathbf{0} + v = v, \text{ for all } v \in \U;}

The following guarentees that addition, as defined, is a valid operation in $\U$, which is a requirement for $\U$ to be a vector space:
\mathdiv{\text{\textbf{Closure Under Addition: }} u, w \in \U \implies u + w \in \U}

The following guarentees that scalar multiplication, as defined, is a valid operation in $\U$, which is a requirement for $\U$ to be a vector space:
\mathdiv{\text{\textbf{Closure Under Scalar Multiplication: }}u \in \U \implies \lambda \cdot u \in \U \text{ for every }\lambda \in \textbf{F}}

According to the theorem on the number (-1) as a scalar, the third test checks off the last required property that $\U$ must satisfy for it to be considered, by definition, a vector space ($\forall v \in \V, \exists -v \in \V$ such that $v + (-v) = \mathbf{0}$). Hence, these three tests are necessary and sufficient to show that a subset $\U$ of $\V$ is a subspace of $\V$ over \textbf{F}.
}
\clearpage

\vspace{10 pt}
Given the last result, it is easy to verify that each of the following are subspaces.
\example{Subspaces}
{
\points
	{
	If $b \in \F{}$, then \mathdiv{ \{(x_1, x_2, x_3, x_4) \in \F{4} : x_3 = 5x_4 + b\} }
    	is a subspace of $\F{4}$ if and only if $b=0.$
	}
    	{The set of continuous real-vaued functions on the interval $[0, 1]$ is a subspace of $\R{[0,1]}$.}
    	{The set of differentiable real-valued functions on $\R{}$ is a subspace of $\R{\R{}}$.}
    	{The set of differentiable real-valued functions $f$ on the interval $(0, 3)$ such that $f'(2)=b$ is a subspace of $\R{(0, 3)}$ if and only if $b = 0$.}
    	{The set of all sequences of complex numbers with limit 0 is a subspace of $\C{\infty}$.}
}

The subspaces of $\R{2}$ are $\{0\}, \R{2}$ and all lines that pass through the origin. The subspaces of $\R{3}$ are $\{0\}, \R{3}$, and all lines and planes that pass through the origin. To prove all these objects are indeed subspaces is easy; to show they are the only subspaces requires tools which are introduced in the next chapter.

The notion of the sum of subspaces will be useful. 

\definition{Sum of Subsets}
{
Suppose $\U_1, \ldots, \U_m \subset \V$. The \textbf{sum} of $\U_1, \ldots, \U_m$, denoted $\U_1 + \cdots + \U_m$, is the set of all possible sums of elements of $\U_1, \ldots, \U_m$. More precisely,
\mathdiv{ \U_1 + \cdots + \U_m = \{u_1 + \cdots + u_m \in \V : u_1 \in \U_1, \ldots, u_m \in \U_m \} }
}

\example{Sum of Subspaces}
{
Let $U \subset \F{3}$ be the set of all elements whose second and third coordinates equal $0$ and $W \subset \F{3}$ be the set of all elements whose first and third coordinates equal $0$:
\mathdiv{U=\{(x, 0, 0) \in \F{3} : x \in \F{}\} \text{ and } W = \{(0, x, 0) \in \F{3} : x \in \F{}\}}
Then,
\mathdiv{U+W = \{(x, y, 0) \in \F{3} : x, y \in \F{}\}}
}
\example{Sum of Subspaces}
{
Let 
\mathdiv{U = \{(x,x,y,y)\in \F{4}: x, y \in \F{}\}\text{ and }W = \{(x,x,x,y) \in \F{4}: x, y \in \F{}\}}
Then,
\mathdiv{U+W = \{(x, x, y, z) \in \F{4} : x, y, z \in \F{}\}}
}
\vspace{10 pt}

\theorem{Sum of subspaces is the smallest containing subspace}
{Suppose $\U_1, \ldots, \U_m$ are subspaces of $\V$. Then $\U_1 + \cdots + \U_m$ is the smallest subspace containing $\U_1, \ldots, \U_m$.}
{}

\subsubsection{Direct Sums}
Consider any $\mathbf{u}$ in $\U_1 + \cdots + \U_m$. By definition, $\exists u_1 \in \U_1, \ldots, \exists u_m \in \U_m$ such that:
\mathdiv{\mathbf{u} = u_1 + \cdots + u_m}
In all kinds of settings, it is ideal to work with sums of subspaces where each $\mathbf{u}$ in $\U_1 + \cdots + \U_m$ can be written as a unique combination of elements. The situation is so important that we give it a special name: \textbf{direct sum}.

\definition{Direct Sum}
{
Suppose $\U_1, \ldots, \U_m$ are subspaces of $\V$.
\points
{The sum $\U_1 + \cdots + \U_m$ is called a \textbf{direct sum} if each element of $\U_1 + \cdots + \U_m$ can be written in only one way as a sum $\u_1 + \cdots + \u_m$, where each $\u_j$ is in $\U_j$.}
{If $\U_1 + \cdots + \U_m$ is a direct sum, then $\U_1 \oplus \cdots \oplus \U_m$ denotes $\U_1 + \cdots + \U_m$ with the notiation $\oplus$ denoting the set is a direct sum.}
}

\example{Natural Basis}
{
Suppose $U_j \subset \F{n}$ is the subspace whose coordinates are all zero, except at coordinate $j$. Hence $U_2 = \{(0,x,0, \ldots, 0) \in \F{n}: x \in \F{}\}$. Then 
\mathdiv{\F{n} = U_1 + \cdots + U_n}
}

\example{Not a direct sum}
{
Let \begin{align*}
U_1 &= \{(x,y,0) \in \F{3} : x, y \in \F{}\}, \\
U_2 &= \{(0,0,z) \in \F{3} : z \in \F{}\}, \\
U_3 &= \{(0,y,y) \in \F{3} : y \in \F{}\} 
\end{align*}
Notice $(0, 0, 0) \in U_1, U_2, U_3$. \\
Also notice \begin{align*}
&(0, 1, 0) \in U_1 \\
&(0,0,1) \in U_2 \\ 
&(0, -1, -1) \in U_3.
\end{align*}
Because $(0,0,0)$ can be written in two different ways as the sum of elements in $U_1, U_2, U_3$,
\mathdiv{(0,0,0) = (0,0,0) + (0,0,0) + (0,0,0) = (0, 1, 0) + (0,0,1) + (0,-1,-1),}
the sum $U_1 + U_2 + U_3$ is not a direct sum.
}


\theorem{Conditions for a direct sum}
{Suppose $\U_1, \ldots, \U_m$ are subspaces of $\V$. Then $\U_1 + \cdots + \U_m$ is a direct sum if and only if the only way to write 0 as a sum of $\u_1 + \cdots + \u_m$, where each $\u_j \in \U_j$ is by taking each $\u_j$ equal to 0.}
{}

\theorem{Conditions for a direct sum}
{Suppose $\U$ and $\W$ are subspaces of $\V$. Then $\U + \W$ is a direct sum if and only if $\U \cap \W = \{0\}$}
{}
\end{document}

















